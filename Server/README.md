# Nexarch Backend

Production-grade FastAPI backend for Architecture Intelligence platform.

## Features

- **Telemetry Ingestion**: Accept and store spans from Nexarch SDK
- **Architecture Reconstruction**: Build dependency graphs using NetworkX
- **Metrics Computation**: Calculate latency, error rate, RPS per node/edge
- **Issue Detection**: Rule-based detection of architectural problems
- **Workflow Generation**: Generate 3 alternative improvement workflows using Azure OpenAI + LangChain
- **REST APIs**: Clean, MCP-ready endpoints

## Architecture

```
Server/
├── main.py                 # FastAPI app
├── api/                    # REST endpoints
│   ├── ingest.py          # POST /api/v1/ingest
│   ├── architecture.py    # GET /api/v1/architecture/current
│   ├── workflows.py       # GET /api/v1/workflows/generated
│   └── health.py          # GET /api/v1/health
├── services/              # Business logic
│   ├── ingest_service.py
│   ├── graph_service.py
│   ├── metrics_service.py
│   ├── issue_detector.py
│   └── workflow_generator.py
├── db/                    # Database
│   ├── base.py
│   └── models.py
├── schemas/               # Pydantic models
├── core/                  # Config & logging
└── utils/                 # AI client
```

## Setup

### 1. Install Dependencies

```bash
cd Server
pip install -r requirements.txt
```

### 2. Configure Environment

Create `.env` file in `Server/` directory:

```env
# Database (optional, defaults to SQLite)
DATABASE_URL=sqlite:///./nexarch.db

# Azure OpenAI (required for workflow generation)
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_DEPLOYMENT=your-deployment-name
AZURE_OPENAI_API_VERSION=2025-01-01-preview

# App Settings (optional)
DEBUG=false

# Thresholds (optional, defaults shown)
HIGH_LATENCY_THRESHOLD_MS=1000
HIGH_ERROR_RATE_THRESHOLD=0.05
MAX_SYNC_CHAIN_DEPTH=5
MAX_FAN_OUT=10
```

### 3. Run Server

```bash
cd Server
python main.py
```

Server runs on `http://localhost:8000`

## API Endpoints

### Health Check

```http
GET /api/v1/health
```

### Ingest Telemetry

```http
POST /api/v1/ingest

{
  "trace_id": "abc123",
  "span_id": "span1",
  "parent_span_id": null,
  "service_name": "api-gateway",
  "operation": "GET /users",
  "kind": "server",
  "start_time": "2026-01-16T10:00:00Z",
  "end_time": "2026-01-16T10:00:01Z",
  "latency_ms": 1000,
  "status_code": 200,
  "error": null,
  "downstream": "user-service"
}
```

### Get Architecture

```http
GET /api/v1/architecture/current
```

Returns:

- `nodes[]`: Services with metrics
- `edges[]`: Call relationships with metrics
- `metrics_summary`: Global stats

### Get Issues

```http
GET /api/v1/architecture/issues
```

Returns detected issues:

- High latency edges
- Deep sync chains
- High error rate nodes
- Fan-out overload

### Get Workflows

```http
GET /api/v1/workflows/generated
```

Returns 3 workflow alternatives:

1. **Minimal Change**: Quick fixes, low risk
2. **Performance Optimized**: Max throughput
3. **Cost Optimized**: Reduce operational cost

### Compare Workflows

```http
GET /api/v1/workflows/comparison
```

Returns workflows with comparison matrix and recommendation.

## Design Principles

1. **No business logic in routes**: All logic in service layer
2. **MCP-ready**: JSON serializable, stateless services
3. **Pydantic everywhere**: Strict validation
4. **Async by default**: FastAPI async patterns
5. **Production-grade**: Proper logging, error handling, validation

## Issue Detection Rules

- **High Latency Edge**: `latency > HIGH_LATENCY_THRESHOLD_MS`
- **Deep Sync Chain**: Call depth > `MAX_SYNC_CHAIN_DEPTH`
- **High Error Rate**: Error rate > `HIGH_ERROR_RATE_THRESHOLD`
- **Fan-out Overload**: Out-degree > `MAX_FAN_OUT`

## Database

Uses SQLite by default. For production, use PostgreSQL:

```env
DATABASE_URL=postgresql://user:pass@localhost/nexarch
```

Tables:

- `spans`: Raw telemetry data
- `architecture_snapshots`: Historical snapshots (future use)

## Development

```bash
# Run with auto-reload
python main.py

# Or with uvicorn directly
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

## Testing

Send sample span:

```bash
curl -X POST http://localhost:8000/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "trace_id": "trace1",
    "span_id": "span1",
    "service_name": "api-gateway",
    "operation": "GET /users",
    "kind": "server",
    "start_time": "2026-01-16T10:00:00Z",
    "end_time": "2026-01-16T10:00:01Z",
    "latency_ms": 1200,
    "status_code": 200,
    "downstream": "user-service"
  }'
```

Get architecture:

```bash
curl http://localhost:8000/api/v1/architecture/current
```

## Next Steps

- [ ] Add authentication
- [ ] Implement MCP server
- [ ] Add background processing
- [ ] Add retention policies
- [ ] Add WebSocket for real-time updates
- [ ] Add Prometheus metrics export

## License

MIT
